{'sample_max_value', 'clip_sample_range', 'timestep_spacing', 'thresholding', 'variance_type', 'rescale_betas_zero_snr', 'prediction_type', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.
{'shift_factor', 'latents_mean', 'force_upcast', 'latents_std', 'use_post_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'use_quant_conv'} was not found in config. Values will be initialized to default values.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
{'time_embedding_type', 'mid_block_type', 'class_embeddings_concat', 'attention_type', 'projection_class_embeddings_input_dim', 'reverse_transformer_layers_per_block', 'conv_in_kernel', 'dropout', 'time_embedding_act_fn', 'time_embedding_dim', 'resnet_out_scale_factor', 'addition_time_embed_dim', 'resnet_skip_time_act', 'num_attention_heads', 'transformer_layers_per_block', 'addition_embed_type', 'mid_block_only_cross_attention', 'cross_attention_norm', 'resnet_time_scale_shift', 'time_cond_proj_dim', 'addition_embed_type_num_heads', 'timestep_post_act', 'class_embed_type', 'upcast_attention', 'encoder_hid_dim', 'conv_out_kernel', 'encoder_hid_dim_type', 'num_class_embeds', 'use_linear_projection', 'dual_cross_attention', 'only_cross_attention'} was not found in config. Values will be initialized to default values.
All model checkpoint weights were used when initializing UNet2DConditionModel.

All the weights of UNet2DConditionModel were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.
Total trainable LoRA parameters: 6377472
Traceback (most recent call last):
  File "C:\Users\isaks\OneDrive - Danmarks Tekniske Universitet\DTU\10semester\Adv_DL_CV\project\SD_finetuning_train.py", line 473, in <module>
    main()
  File "C:\Users\isaks\OneDrive - Danmarks Tekniske Universitet\DTU\10semester\Adv_DL_CV\project\SD_finetuning_train.py", line 248, in main
    optimizer = torch.optim.AdamW(
                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\optim\adamw.py", line 77, in __init__
    super().__init__(params, defaults)
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\optim\optimizer.py", line 366, in __init__
    raise ValueError("optimizer got an empty parameter list")
ValueError: optimizer got an empty parameter list
