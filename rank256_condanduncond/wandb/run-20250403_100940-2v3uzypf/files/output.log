{'dynamic_thresholding_ratio', 'timestep_spacing', 'variance_type', 'prediction_type', 'rescale_betas_zero_snr', 'clip_sample_range', 'sample_max_value', 'thresholding'} was not found in config. Values will be initialized to default values.
{'use_quant_conv', 'latents_std', 'scaling_factor', 'use_post_quant_conv', 'force_upcast', 'shift_factor', 'latents_mean', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
{'dual_cross_attention', 'time_embedding_dim', 'mid_block_type', 'resnet_skip_time_act', 'only_cross_attention', 'use_linear_projection', 'time_embedding_type', 'transformer_layers_per_block', 'conv_out_kernel', 'dropout', 'class_embeddings_concat', 'addition_embed_type_num_heads', 'addition_time_embed_dim', 'class_embed_type', 'time_cond_proj_dim', 'addition_embed_type', 'reverse_transformer_layers_per_block', 'mid_block_only_cross_attention', 'num_attention_heads', 'projection_class_embeddings_input_dim', 'resnet_time_scale_shift', 'conv_in_kernel', 'num_class_embeds', 'time_embedding_act_fn', 'timestep_post_act', 'encoder_hid_dim_type', 'cross_attention_norm', 'attention_type', 'resnet_out_scale_factor', 'encoder_hid_dim', 'upcast_attention'} was not found in config. Values will be initialized to default values.
All model checkpoint weights were used when initializing UNet2DConditionModel.

All the weights of UNet2DConditionModel were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.
Total trainable LoRA parameters: 51019776
***** Running training *****
  Num examples = 223414
  Num Epochs = 1
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 2
  Total optimization steps = 5000
Resuming full training state from rank256_condanduncond\checkpoint-2000
Traceback (most recent call last):
  File "C:\Users\isaks\OneDrive - Danmarks Tekniske Universitet\DTU\10semester\Adv_DL_CV\project\SD_finetuning_train.py", line 477, in <module>
    main()
  File "C:\Users\isaks\OneDrive - Danmarks Tekniske Universitet\DTU\10semester\Adv_DL_CV\project\SD_finetuning_train.py", line 336, in main
    accelerator.load_state(checkpoint_path)
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\accelerate\accelerator.py", line 3452, in load_state
    override_attributes = load_accelerator_state(
                          ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\accelerate\checkpointing.py", line 226, in load_accelerator_state
    load_model(model, input_model_file, device=str(map_location), **load_model_func_kwargs)
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\safetensors\torch.py", line 202, in load_model
    state_dict = load_file(filename, device=device)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\safetensors\torch.py", line 313, in load_file
    with safe_open(filename, framework="pt", device=device) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer
