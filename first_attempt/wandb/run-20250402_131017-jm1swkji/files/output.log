{'sample_max_value', 'variance_type', 'clip_sample_range', 'prediction_type', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio', 'thresholding', 'timestep_spacing'} was not found in config. Values will be initialized to default values.
{'scaling_factor', 'use_post_quant_conv', 'shift_factor', 'latents_std', 'mid_block_add_attention', 'force_upcast', 'use_quant_conv', 'latents_mean'} was not found in config. Values will be initialized to default values.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
{'encoder_hid_dim', 'transformer_layers_per_block', 'upcast_attention', 'cross_attention_norm', 'conv_in_kernel', 'mid_block_type', 'timestep_post_act', 'attention_type', 'resnet_skip_time_act', 'only_cross_attention', 'class_embed_type', 'addition_time_embed_dim', 'resnet_time_scale_shift', 'addition_embed_type', 'reverse_transformer_layers_per_block', 'class_embeddings_concat', 'encoder_hid_dim_type', 'projection_class_embeddings_input_dim', 'time_embedding_type', 'time_embedding_dim', 'time_cond_proj_dim', 'addition_embed_type_num_heads', 'use_linear_projection', 'time_embedding_act_fn', 'dropout', 'dual_cross_attention', 'conv_out_kernel', 'num_attention_heads', 'mid_block_only_cross_attention', 'resnet_out_scale_factor', 'num_class_embeds'} was not found in config. Values will be initialized to default values.
All model checkpoint weights were used when initializing UNet2DConditionModel.

All the weights of UNet2DConditionModel were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.
***** Running training *****
  Num examples = 223414
  Num Epochs = 1
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 5000
Steps:   0%|                                                                                                                                                                                     | 0/5000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "C:\Users\isaks\OneDrive - Danmarks Tekniske Universitet\DTU\10semester\Adv_DL_CV\project\SD_finetuning_train.py", line 444, in <module>
    main()
  File "C:\Users\isaks\OneDrive - Danmarks Tekniske Universitet\DTU\10semester\Adv_DL_CV\project\SD_finetuning_train.py", line 345, in main
    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\accelerate\utils\operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\accelerate\utils\operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\amp\autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\diffusers\models\unets\unet_2d_condition.py", line 1279, in forward
    sample = upsample_block(
             ^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\diffusers\models\unets\unet_2d_blocks.py", line 2458, in forward
    hidden_states = attn(
                    ^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\diffusers\models\transformers\transformer_2d.py", line 427, in forward
    hidden_states = block(
                    ^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\diffusers\models\attention.py", line 495, in forward
    norm_hidden_states = self.norm1(hidden_states)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\modules\normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\nn\functional.py", line 2900, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 10.00 GiB of which 0 bytes is free. Of the allocated memory 16.44 GiB is allocated by PyTorch, and 75.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
