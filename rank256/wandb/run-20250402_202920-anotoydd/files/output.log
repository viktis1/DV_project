{'dynamic_thresholding_ratio', 'clip_sample_range', 'timestep_spacing', 'variance_type', 'thresholding', 'sample_max_value', 'prediction_type', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.
{'shift_factor', 'use_quant_conv', 'mid_block_add_attention', 'use_post_quant_conv', 'latents_mean', 'scaling_factor', 'force_upcast', 'latents_std'} was not found in config. Values will be initialized to default values.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
{'addition_embed_type_num_heads', 'addition_embed_type', 'class_embed_type', 'transformer_layers_per_block', 'timestep_post_act', 'mid_block_only_cross_attention', 'projection_class_embeddings_input_dim', 'mid_block_type', 'resnet_skip_time_act', 'dual_cross_attention', 'addition_time_embed_dim', 'time_embedding_act_fn', 'encoder_hid_dim_type', 'num_attention_heads', 'time_cond_proj_dim', 'attention_type', 'reverse_transformer_layers_per_block', 'cross_attention_norm', 'resnet_out_scale_factor', 'upcast_attention', 'time_embedding_dim', 'conv_out_kernel', 'time_embedding_type', 'use_linear_projection', 'only_cross_attention', 'conv_in_kernel', 'class_embeddings_concat', 'encoder_hid_dim', 'num_class_embeds', 'resnet_time_scale_shift', 'dropout'} was not found in config. Values will be initialized to default values.
All model checkpoint weights were used when initializing UNet2DConditionModel.

All the weights of UNet2DConditionModel were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.
Total trainable LoRA parameters: 51019776
***** Running training *****
  Num examples = 223414
  Num Epochs = 1
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 2
  Total optimization steps = 10000
Resuming from LoRA weights: rank256\lora-steps-1500\pytorch_lora_weights.safetensors
C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\diffusers\loaders\unet.py:212: FutureWarning: `load_attn_procs` is deprecated and will be removed in version 0.40.0. Using the `load_attn_procs()` method has been deprecated and will be removed in a future version. Please use `load_lora_adapter()`.
  deprecate("load_attn_procs", "0.40.0", deprecation_message)
C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\peft\tuners\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
Steps:  15%|███████████████████████▎                                                                                                                                   | 1500/10000 [00:02<?, ?it/s, loss=0.64, lr=0.0001]Traceback (most recent call last):
  File "C:\Users\isaks\OneDrive - Danmarks Tekniske Universitet\DTU\10semester\Adv_DL_CV\project\SD_finetuning_train.py", line 476, in <module>
    main()
  File "C:\Users\isaks\OneDrive - Danmarks Tekniske Universitet\DTU\10semester\Adv_DL_CV\project\SD_finetuning_train.py", line 390, in main
    optimizer.step()
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\accelerate\optimizer.py", line 165, in step
    self.scaler.step(self.optimizer, closure)
  File "C:\Users\isaks\miniconda3\envs\adv_dl_cv\Lib\site-packages\torch\amp\grad_scaler.py", line 454, in step
    len(optimizer_state["found_inf_per_device"]) > 0
AssertionError: No inf checks were recorded for this optimizer.
